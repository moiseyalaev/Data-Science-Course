{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as lin\n",
    "from scipy.linalg import svdvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_gradDescent(A, b):\n",
    "    x_curr = np.random.randn(50) # set an intal x of random vals \n",
    "    \n",
    "    # take alpha to be L := 2*sigma_1(A)^2\n",
    "    firstSingVal = svdvals(A)[0]\n",
    "    L = 2*firstSingVal**2 \n",
    "    a = L**-1\n",
    "    print(a)\n",
    "    # if a is larger than L^-1 then convergence is slower and could cause\n",
    "    # the algorithm to skip over the global min. However, you can see this immediatly\n",
    "    # by taking a = .004 which is atleast .001 greater than most L^-1 vals.\n",
    "    \n",
    "    for k in range(100):\n",
    "        x_next = x_curr - a * grad(A, b, x_curr)\n",
    "        x_curr  = x_next\n",
    "        if k%10 == 0 or k == 99: # print the last 11 iterations to see what it converges to\n",
    "            print('At k =', k, 'The error is:', lin.norm(np.dot(A, x_curr) - b)**2)\n",
    "    \n",
    "    return x_curr\n",
    "\n",
    "def grad(A, b, x):\n",
    "    return 2 * np.dot(A.T, (np.dot(A,x) - b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0017276671182729852\n",
      "At k = 0 The error is: 1765.5334831638745\n",
      "At k = 10 The error is: 75.1814467307625\n",
      "At k = 20 The error is: 41.5608083729421\n",
      "At k = 30 The error is: 35.484333147664536\n",
      "At k = 40 The error is: 33.87514883259244\n",
      "At k = 50 The error is: 33.37746481925921\n",
      "At k = 60 The error is: 33.20920265565529\n",
      "At k = 70 The error is: 33.14827408875669\n",
      "At k = 80 The error is: 33.12485010678034\n",
      "At k = 90 The error is: 33.115361378935845\n",
      "At k = 99 The error is: 33.11161422846379\n"
     ]
    }
   ],
   "source": [
    "# We know that \n",
    "A = np.random.randn(100,50)\n",
    "b = np.random.randn(100)\n",
    "x = my_gradDescent(A, b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
